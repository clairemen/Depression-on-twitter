{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing import Process\n",
    "from empath import Empath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textblob\n",
    "import spacy\n",
    "import time\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "prog = re.compile(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t,.?!])|(\\w+:\\/\\/\\S+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatization(x, nlp):\n",
    "    lemmatized = []\n",
    "    for tweet in list(x.values):\n",
    "        s=\"\"\n",
    "        letters_only = prog.sub(\" \", tweet)        \n",
    "        s=\" \".join([token1.lemma_ for token1 in nlp(letters_only) if token1 not in stopWords and token1.text.lower()!='rt' and token1.lemma_ != \"-PRON-\"])\n",
    "        lemmatized.append(s)\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processing(vals, columns, iterv):\n",
    "    users = pd.DataFrame(vals)\n",
    "    users = users[columns]\n",
    "    print(\"{0}-------------\".format(iterv))\n",
    "    # PRE-PROCESSING\n",
    "    users_text = users.groupby([\"uid\"])[\"tweet\"].apply(lambda x: lemmatization(x, nlp)).reset_index()\n",
    "    print(\"{0}-------------PRE-PROCESSING\".format(iterv))\n",
    "    users_text.to_csv(\"G:\\\\sufe\\\\mental illness\\\\Dataset\\\\labeled\\\\positive\\\\data\\\\user_tweets_new\\\\users_tweets_{0}.csv\".format(iterv))\n",
    "    print(\"-------------{0}\".format(iterv))\n",
    "#print(data_df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-------------\n",
      "1-------------PRE-PROCESSING\n",
      "-------------1\n",
      "804.1889972686768\n",
      "50366\n",
      "2\n",
      "2-------------\n",
      "2-------------PRE-PROCESSING\n",
      "-------------2\n",
      "809.0852770805359\n",
      "100579\n",
      "3\n",
      "3-------------\n",
      "3-------------PRE-PROCESSING\n",
      "-------------3\n",
      "778.3415186405182\n",
      "150708\n",
      "4\n",
      "4-------------\n",
      "4-------------PRE-PROCESSING\n",
      "-------------4\n",
      "780.6686518192291\n",
      "200732\n",
      "5\n",
      "5-------------\n",
      "5-------------PRE-PROCESSING\n",
      "-------------5\n",
      "785.3609199523926\n",
      "251709\n",
      "6\n",
      "6-------------\n",
      "6-------------PRE-PROCESSING\n",
      "-------------6\n",
      "783.0497877597809\n",
      "302129\n",
      "7\n",
      "7-------------\n",
      "7-------------PRE-PROCESSING\n",
      "-------------7\n",
      "783.3968079090118\n",
      "352351\n",
      "8\n",
      "8-------------\n",
      "8-------------PRE-PROCESSING\n",
      "-------------8\n",
      "805.7280850410461\n",
      "402900\n",
      "9\n",
      "9-------------\n",
      "9-------------PRE-PROCESSING\n",
      "-------------9\n",
      "801.4518404006958\n",
      "453021\n",
      "10\n",
      "10-------------\n",
      "10-------------PRE-PROCESSING\n",
      "-------------10\n",
      "133.80265283584595\n"
     ]
    }
   ],
   "source": [
    "f =open('G:\\\\sufe\\\\mental illness\\\\Dataset\\\\labeled\\\\positive\\\\data\\\\positive_tweet.csv','r',encoding='utf-8')\n",
    "cols = ['favorite_count', 'favorited', 'id', 'in_reply_to_status_id', 'lang', 'retweet_count', 'retweeted', 'time', 'tweet', 'uid']\n",
    "csv_dict_reader = csv.DictReader(f)\n",
    "acc_vals = []\n",
    "iter_vals, count, count_max, last_u, v = 1, 0, 50000, None, []\n",
    "last_u=None\n",
    "i=0\n",
    "#iter_vals=\n",
    "for line in csv_dict_reader:\n",
    "    i +=1\n",
    "    if(i<0):\n",
    "        pass\n",
    "    else:\n",
    "        if last_u is not None and last_u != line[\"uid\"]:\n",
    "            s = time.time()\n",
    "            processing(v, cols, iter_vals)\n",
    "            print(time.time() - s)\n",
    "            print(i)\n",
    "            count, last_u, v = 0, None, []\n",
    "            iter_vals += 1\n",
    "            print(iter_vals)\n",
    "        v.append(line)\n",
    "        count += 1\n",
    "        if count >= count_max:\n",
    "            last_u = line[\"uid\"]\n",
    "        else:\n",
    "            pass\n",
    "s = time.time()\n",
    "processing(v, cols, iter_vals)\n",
    "print(time.time() - s)\n",
    "v=[]\n",
    "last_u=None\n",
    "iter_vals=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
