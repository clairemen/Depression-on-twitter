{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing import Process\n",
    "from empath import Empath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textblob\n",
    "import spacy\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "data_df = pd.read_csv('G:\\\\sufe\\\\mental illness\\\\Dataset\\\\labeled\\\\positive\\\\data\\\\positive_tweet.csv')\n",
    "#删除网址  mention 停用词\n",
    "tweet_text = data_df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "prog = re.compile(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z' \\t])|(\\w+:\\/\\/\\S+)\")\n",
    "prog2 = re.compile(\" +\")\n",
    "lexicon = Empath()\n",
    "empath_cols = [\"{0}_empath\".format(v) for v in lexicon.cats.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(x, nlp):\n",
    "    tweets = \" \".join(list(x.values))\n",
    "    letters_only = prog.sub(\" \", tweets)\n",
    "    lemmatized = []\n",
    "    for token1 in nlp(letters_only):\n",
    "        if token1.lemma_ != \"-PRON-\" and token1 not in stopWords:\n",
    "            lemmatized.append(token1.lemma_)\n",
    "        else:\n",
    "            lemmatized.append(token1.text)\n",
    "    final = prog2.sub(\" \", \" \".join(lemmatized))\n",
    "    return final\n",
    "\n",
    "\n",
    "def empath_analysis(x):\n",
    "    val = lexicon.analyze(x, normalize=True)\n",
    "    if val is None:\n",
    "        return lexicon.analyze(x)\n",
    "    else:\n",
    "        return val\n",
    "x=data_df['tweet'][:20]\n",
    "#print(lemmatization(x,nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(vals, columns, iterv):\n",
    "    users = pd.DataFrame(vals)\n",
    "    users = users[columns]\n",
    "\n",
    "    print(\"{0}-------------\".format(iterv))\n",
    "\n",
    "    # PRE-PROCESSING\n",
    "\n",
    "    users_text = users.groupby([\"uid\"])[\"tweet\"].apply(lambda x: lemmatization(x, nlp)).reset_index()\n",
    "    print(\"{0}-------------PRE-PROCESSING\".format(iterv))\n",
    "\n",
    "    # SENTIMENT ANALYSIS\n",
    "\n",
    "    sentiment_arr = np.array(list(users_text[\"tweet\"].apply(lambda x: textblob.TextBlob(str(x)).sentiment).values))\n",
    "    sentiment_cols = [\"sentiment\", \"subjectivity\"]\n",
    "    df_sentiment = pd.DataFrame(sentiment_arr, columns=sentiment_cols, index=users_text.uid.values)\n",
    "    print(\"{0}-------------SENTIMENT\".format(iterv))\n",
    "\n",
    "    # EMPATH ANALYSIS\n",
    "\n",
    "    lexicon_arr = np.array(list(users_text[\"tweet\"].apply(lambda x: empath_analysis(x)).values))\n",
    "    df_empath = pd.DataFrame.from_records(index=users_text.uid.values, data=lexicon_arr)\n",
    "    df_empath.columns = empath_cols\n",
    "    print(\"{0}-------------EMPATH\".format(iterv))\n",
    "\n",
    "    # MERGE TO SINGLE\n",
    "\n",
    "    df = pd.DataFrame(pd.concat([df_empath, df_sentiment], axis=1))\n",
    "    #df.set_index(\"uid\", inplace=True)\n",
    "    df.to_csv(\"G:\\\\sufe\\\\mental illness\\\\Dataset\\\\labeled\\\\positive\\\\data\\\\users_content_{0}.csv\".format(iterv))\n",
    "    print(\"-------------{0}\".format(iterv))\n",
    "#print(data_df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-------------\n",
      "1-------------PRE-PROCESSING\n",
      "1-------------SENTIMENT\n",
      "1-------------EMPATH\n",
      "-------------1\n",
      "174.48697996139526\n",
      "2-------------\n",
      "2-------------PRE-PROCESSING\n",
      "2-------------SENTIMENT\n",
      "2-------------EMPATH\n",
      "-------------2\n",
      "185.12958884239197\n",
      "3-------------\n",
      "3-------------PRE-PROCESSING\n",
      "3-------------SENTIMENT\n",
      "3-------------EMPATH\n",
      "-------------3\n",
      "180.23130869865417\n",
      "4-------------\n",
      "4-------------PRE-PROCESSING\n",
      "4-------------SENTIMENT\n",
      "4-------------EMPATH\n",
      "-------------4\n",
      "179.71827936172485\n",
      "5-------------\n",
      "5-------------PRE-PROCESSING\n",
      "5-------------SENTIMENT\n",
      "5-------------EMPATH\n",
      "-------------5\n",
      "182.392431974411\n",
      "6-------------\n",
      "6-------------PRE-PROCESSING\n",
      "6-------------SENTIMENT\n",
      "6-------------EMPATH\n",
      "-------------6\n",
      "177.95917868614197\n",
      "7-------------\n",
      "7-------------PRE-PROCESSING\n",
      "7-------------SENTIMENT\n",
      "7-------------EMPATH\n",
      "-------------7\n",
      "176.90411853790283\n",
      "8-------------\n",
      "8-------------PRE-PROCESSING\n",
      "8-------------SENTIMENT\n",
      "8-------------EMPATH\n",
      "-------------8\n",
      "180.59932947158813\n",
      "9-------------\n",
      "9-------------PRE-PROCESSING\n",
      "9-------------SENTIMENT\n",
      "9-------------EMPATH\n",
      "-------------9\n",
      "172.62987399101257\n",
      "10-------------\n",
      "10-------------PRE-PROCESSING\n",
      "10-------------SENTIMENT\n",
      "10-------------EMPATH\n",
      "-------------10\n",
      "27.400567293167114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ns = time.time()\\nprocesses = []\\nfor i in acc_vals:\\n    p = Process(target=processing, args=(i[0], i[1], i[2]))\\n    processes.append(p)\\nfor p in processes:\\n    p.start()\\nfor p in processes:\\n    p.join()\\nprint(time.time() - s)\\nacc_vals = []\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f =open('G:\\\\sufe\\\\mental illness\\\\Dataset\\\\labeled\\\\positive\\\\data\\\\positive_tweet.csv','r',encoding='utf-8')\n",
    "cols = ['favorite_count', 'favorited', 'id', 'in_reply_to_status_id', 'lang', 'retweet_count', 'retweeted', 'time', 'tweet', 'uid']\n",
    "csv_dict_reader = csv.DictReader(f)\n",
    "\n",
    "acc_vals = []\n",
    "iter_vals, count, count_max, last_u, v = 1, 0, 50000, None, []\n",
    "last_u=None\n",
    "for line in csv_dict_reader:\n",
    "    if last_u is not None and last_u != line[\"uid\"]:\n",
    "        #提取特征\n",
    "        s = time.time()\n",
    "        processing(v, cols, iter_vals)\n",
    "        print(time.time() - s)        \n",
    "        count, last_u, v = 0, None, []\n",
    "        iter_vals += 1\n",
    "    v.append(line)\n",
    "    count += 1\n",
    "    if count >= count_max:\n",
    "        last_u = line[\"uid\"]\n",
    "\n",
    "s = time.time()\n",
    "processing(v, cols, iter_vals)\n",
    "print(time.time() - s)\n",
    "v=[]\n",
    "last_u=None\n",
    "iter_vals=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
